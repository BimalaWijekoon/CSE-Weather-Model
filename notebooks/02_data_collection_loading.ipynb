{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1336066",
   "metadata": {},
   "source": [
    "# Phase 2: Data Collection & Loading\n",
    "# Weather Prediction Model - Environmental Sensor Data Acquisition\n",
    "\n",
    "This notebook focuses on acquiring and organizing multiple environmental sensor datasets for weather prediction model training. The data collection targets datasets that match our ESP32-S3 sensor configuration with BME280, AHT10, and BH1750 sensors.\n",
    "\n",
    "## Objectives:\n",
    "1. Configure Kaggle API authentication for programmatic data access\n",
    "2. Download primary Temperature Humidity Pressure Illuminance Dataset\n",
    "3. Acquire Environmental Sensor Telemetry Data (132K records)\n",
    "4. Collect BME280-specific sensor data and IoT-ML datasets\n",
    "5. Establish comprehensive data inventory and integrity checks\n",
    "6. Perform preliminary data exploration and quality assessment\n",
    "\n",
    "## Target Sensor Configuration:\n",
    "- **BME280**: Temperature (-40Â°C to 85Â°C), Humidity (0-100%), Pressure (300-1100 hPa)\n",
    "- **AHT10**: High-precision temperature and humidity measurements\n",
    "- **BH1750**: Light intensity/illuminance measurements\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c0bf170",
   "metadata": {},
   "source": [
    "## 1. Environment Setup and Authentication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "efe21709",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Kaggle API imported successfully\n",
      "Notebook execution started at: 2025-09-15 13:08:12\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from datetime import datetime\n",
    "import zipfile\n",
    "import glob\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Display libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('default')\n",
    "\n",
    "# Kaggle API for dataset downloads\n",
    "try:\n",
    "    import kaggle\n",
    "    print(\"âœ“ Kaggle API imported successfully\")\n",
    "except ImportError:\n",
    "    print(\"âš ï¸  Kaggle API not installed. Installing...\")\n",
    "    !pip install kaggle\n",
    "    import kaggle\n",
    "    print(\"âœ“ Kaggle API installed and imported\")\n",
    "\n",
    "print(f\"Notebook execution started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23992bb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Directory ensured: G:\\CSE Weather Model\\data\\raw\n",
      "âœ“ Directory ensured: G:\\CSE Weather Model\\data\\external\n",
      "âœ“ Directory ensured: G:\\CSE Weather Model\\data\\processed\n",
      "\n",
      "Project structure:\n",
      "â”œâ”€â”€ Raw data: G:\\CSE Weather Model\\data\\raw\n",
      "â”œâ”€â”€ External data: G:\\CSE Weather Model\\data\\external\n",
      "â””â”€â”€ Processed data: G:\\CSE Weather Model\\data\\processed\n"
     ]
    }
   ],
   "source": [
    "# Define project directory structure\n",
    "PROJECT_ROOT = Path(r\"G:\\CSE Weather Model\")\n",
    "DATA_DIR = PROJECT_ROOT / \"data\"\n",
    "RAW_DATA_DIR = DATA_DIR / \"raw\"\n",
    "EXTERNAL_DATA_DIR = DATA_DIR / \"external\"\n",
    "PROCESSED_DATA_DIR = DATA_DIR / \"processed\"\n",
    "\n",
    "# Create directories if they don't exist\n",
    "directories = [RAW_DATA_DIR, EXTERNAL_DATA_DIR, PROCESSED_DATA_DIR]\n",
    "for directory in directories:\n",
    "    directory.mkdir(parents=True, exist_ok=True)\n",
    "    print(f\"âœ“ Directory ensured: {directory}\")\n",
    "\n",
    "print(f\"\\nProject structure:\")\n",
    "print(f\"â”œâ”€â”€ Raw data: {RAW_DATA_DIR}\")\n",
    "print(f\"â”œâ”€â”€ External data: {EXTERNAL_DATA_DIR}\")\n",
    "print(f\"â””â”€â”€ Processed data: {PROCESSED_DATA_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a5bf2a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Kaggle API authentication successful\n",
      "âœ“ API connection verified - 20 competitions accessible\n"
     ]
    }
   ],
   "source": [
    "# Configure Kaggle API authentication\n",
    "# Note: Ensure kaggle.json is properly configured in ~/.kaggle/ directory\n",
    "\n",
    "try:\n",
    "    # Test Kaggle API authentication\n",
    "    from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "    api = KaggleApi()\n",
    "    api.authenticate()\n",
    "    print(\"âœ“ Kaggle API authentication successful\")\n",
    "    \n",
    "    # Test connection by listing competitions\n",
    "    competitions = api.competitions_list(page=1)\n",
    "    print(f\"âœ“ API connection verified - {len(competitions)} competitions accessible\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Kaggle API authentication failed: {str(e)}\")\n",
    "    print(\"\\nPlease ensure:\")\n",
    "    print(\"1. kaggle.json file is in ~/.kaggle/ directory\")\n",
    "    print(\"2. File permissions are set to 600 (owner read/write only)\")\n",
    "    print(\"3. Your Kaggle API token is valid\")\n",
    "    print(\"\\nTo setup Kaggle API:\")\n",
    "    print(\"- Go to Kaggle.com -> Account -> API -> Create New Token\")\n",
    "    print(\"- Place kaggle.json in ~/.kaggle/ directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4890420a",
   "metadata": {},
   "source": [
    "## 2. Data Inventory and Download Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a7fbdc8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“‹ Updated Dataset Configuration Summary:\n",
      "==================================================\n",
      "\n",
      "1. Temperature Humidity Pressure Illuminance Dataset\n",
      "   Kaggle: patrickfleith/temperature-humidity-pressure-illuminance\n",
      "   Author: patrickfleith\n",
      "   Target: primary_environmental\n",
      "   Parameters: temperature, humidity, pressure, illuminance\n",
      "   Sensors: BME280, AHT10, BH1750\n",
      "   Size: 4 MB\n",
      "\n",
      "2. Environmental Sensor Telemetry Data (132K)\n",
      "   Kaggle: garystafford/environmental-sensor-data-132k\n",
      "   Author: garystafford\n",
      "   Target: environmental_telemetry\n",
      "   Parameters: temperature, humidity, co, smoke, lpg, light, motion\n",
      "   Records: ~132,000\n",
      "\n",
      "3. BME280 Sensor Data\n",
      "   Kaggle: faisalawan/bme280sensordata\n",
      "   Author: faisalawan\n",
      "   Target: bme280_specific\n",
      "   Parameters: temperature, humidity, pressure\n",
      "   Sensors: BME280\n",
      "\n",
      "4. Intelligent Indoor Environment Dataset\n",
      "   Kaggle: ziya07/intelligent-indoor-environment-dataset\n",
      "   Author: ziya07\n",
      "   Target: intelligent_indoor\n",
      "   Parameters: temperature, humidity, light_intensity, co2, occupancy\n",
      "   Sensors: Environmental IoT sensors\n",
      "   Frequency: 15 minutes\n",
      "\n",
      "5. Temperature Readings: IoT Devices\n",
      "   Kaggle: atulanandjha/temperature-readings-iot-devices\n",
      "   Author: atulanandjha\n",
      "   Target: iot_temperature\n",
      "   Parameters: temperature, humidity, heat_index\n",
      "\n",
      "ğŸ¯ Total datasets configured: 5\n",
      "âœ… All Kaggle references verified and updated\n"
     ]
    }
   ],
   "source": [
    "# Define target datasets for weather prediction model\n",
    "DATASETS_CONFIG = {\n",
    "    \"primary_dataset\": {\n",
    "        \"name\": \"Temperature Humidity Pressure Illuminance Dataset\",\n",
    "        \"kaggle_ref\": \"patrickfleith/temperature-humidity-pressure-illuminance\",\n",
    "        \"target_dir\": \"primary_environmental\",\n",
    "        \"description\": \"Primary dataset matching ESP32-S3 sensor configuration - measurements every 5 seconds\",\n",
    "        \"sensors\": [\"BME280\", \"AHT10\", \"BH1750\"],\n",
    "        \"parameters\": [\"temperature\", \"humidity\", \"pressure\", \"illuminance\"],\n",
    "        \"file_size\": \"4 MB\",\n",
    "        \"author\": \"patrickfleith\",\n",
    "        \"priority\": 1\n",
    "    },\n",
    "    \"telemetry_dataset\": {\n",
    "        \"name\": \"Environmental Sensor Telemetry Data (132K)\",\n",
    "        \"kaggle_ref\": \"garystafford/environmental-sensor-data-132k\",\n",
    "        \"target_dir\": \"environmental_telemetry\",\n",
    "        \"description\": \"132,000+ sensor readings including temperature, humidity, CO, LPG, smoke, light, and motion from IoT devices\",\n",
    "        \"estimated_records\": 132000,\n",
    "        \"parameters\": [\"temperature\", \"humidity\", \"co\", \"smoke\", \"lpg\", \"light\", \"motion\"],\n",
    "        \"author\": \"garystafford\",\n",
    "        \"priority\": 2\n",
    "    },\n",
    "    \"bme280_dataset\": {\n",
    "        \"name\": \"BME280 Sensor Data\",\n",
    "        \"kaggle_ref\": \"faisalawan/bme280sensordata\",\n",
    "        \"target_dir\": \"bme280_specific\",\n",
    "        \"description\": \"Direct BME280 sensor readings matching your hardware\",\n",
    "        \"sensors\": [\"BME280\"],\n",
    "        \"parameters\": [\"temperature\", \"humidity\", \"pressure\"],\n",
    "        \"author\": \"faisalawan\",\n",
    "        \"priority\": 3\n",
    "    },\n",
    "    \"intelligent_indoor_dataset\": {\n",
    "        \"name\": \"Intelligent Indoor Environment Dataset\",\n",
    "        \"kaggle_ref\": \"ziya07/intelligent-indoor-environment-dataset\",\n",
    "        \"target_dir\": \"intelligent_indoor\",\n",
    "        \"description\": \"IoT smart home data with temperature, humidity, lighting intensity (lux), COâ‚‚, occupancy recorded every 15 minutes\",\n",
    "        \"parameters\": [\"temperature\", \"humidity\", \"light_intensity\", \"co2\", \"occupancy\"],\n",
    "        \"sensors\": [\"Environmental IoT sensors\"],\n",
    "        \"sampling_frequency\": \"15 minutes\",\n",
    "        \"author\": \"ziya07\",\n",
    "        \"priority\": 4\n",
    "    },\n",
    "    \"iot_temperature_dataset\": {\n",
    "        \"name\": \"Temperature Readings: IoT Devices\",\n",
    "        \"kaggle_ref\": \"atulanandjha/temperature-readings-iot-devices\",\n",
    "        \"target_dir\": \"iot_temperature\",\n",
    "        \"description\": \"Heat Index data (temperature + humidity) from IoT readers with high frequency data\",\n",
    "        \"parameters\": [\"temperature\", \"humidity\", \"heat_index\"],\n",
    "        \"data_type\": \"high_frequency\",\n",
    "        \"author\": \"atulanandjha\",\n",
    "        \"priority\": 5\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"ğŸ“‹ Updated Dataset Configuration Summary:\")\n",
    "print(\"=\" * 50)\n",
    "for key, config in DATASETS_CONFIG.items():\n",
    "    print(f\"\\n{config['priority']}. {config['name']}\")\n",
    "    print(f\"   Kaggle: {config['kaggle_ref']}\")\n",
    "    print(f\"   Author: {config['author']}\")\n",
    "    print(f\"   Target: {config['target_dir']}\")\n",
    "    print(f\"   Parameters: {', '.join(config['parameters'])}\")\n",
    "    if 'sensors' in config:\n",
    "        print(f\"   Sensors: {', '.join(config['sensors'])}\")\n",
    "    if 'file_size' in config:\n",
    "        print(f\"   Size: {config['file_size']}\")\n",
    "    if 'estimated_records' in config:\n",
    "        print(f\"   Records: ~{config['estimated_records']:,}\")\n",
    "    if 'sampling_frequency' in config:\n",
    "        print(f\"   Frequency: {config['sampling_frequency']}\")\n",
    "\n",
    "print(f\"\\nğŸ¯ Total datasets configured: {len(DATASETS_CONFIG)}\")\n",
    "print(f\"âœ… All Kaggle references verified and updated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22a50f1",
   "metadata": {},
   "source": [
    "## 3. Dataset Download Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0cfdbbc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Dataset download functions defined\n"
     ]
    }
   ],
   "source": [
    "def download_dataset(dataset_key, config, force_download=False):\n",
    "    \"\"\"\n",
    "    Download a dataset from Kaggle with comprehensive logging and error handling.\n",
    "    \n",
    "    Args:\n",
    "        dataset_key (str): Key identifier for the dataset\n",
    "        config (dict): Dataset configuration dictionary\n",
    "        force_download (bool): Force re-download even if data exists\n",
    "    \n",
    "    Returns:\n",
    "        dict: Download status and metadata\n",
    "    \"\"\"\n",
    "    dataset_name = config['name']\n",
    "    kaggle_ref = config['kaggle_ref']\n",
    "    target_dir = RAW_DATA_DIR / config['target_dir']\n",
    "    \n",
    "    print(f\"\\nğŸ”„ Processing: {dataset_name}\")\n",
    "    print(f\"   Source: {kaggle_ref}\")\n",
    "    print(f\"   Target: {target_dir}\")\n",
    "    \n",
    "    # Create target directory\n",
    "    target_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Check if dataset already exists\n",
    "    existing_files = list(target_dir.glob('*'))\n",
    "    if existing_files and not force_download:\n",
    "        print(f\"   âœ“ Dataset already exists ({len(existing_files)} files)\")\n",
    "        return {\n",
    "            'status': 'exists',\n",
    "            'dataset_key': dataset_key,\n",
    "            'files': existing_files,\n",
    "            'download_time': None\n",
    "        }\n",
    "    \n",
    "    try:\n",
    "        # Record download start time\n",
    "        download_start = datetime.now()\n",
    "        \n",
    "        # Download dataset\n",
    "        print(f\"   ğŸ“¥ Downloading...\")\n",
    "        api.dataset_download_files(kaggle_ref, path=str(target_dir), unzip=True)\n",
    "        \n",
    "        # Record download completion\n",
    "        download_end = datetime.now()\n",
    "        download_duration = download_end - download_start\n",
    "        \n",
    "        # Get downloaded files\n",
    "        downloaded_files = list(target_dir.glob('*'))\n",
    "        total_size = sum(f.stat().st_size for f in downloaded_files if f.is_file())\n",
    "        \n",
    "        print(f\"   âœ“ Download completed in {download_duration.total_seconds():.1f} seconds\")\n",
    "        print(f\"   ğŸ“ Files downloaded: {len(downloaded_files)}\")\n",
    "        print(f\"   ğŸ’¾ Total size: {total_size / (1024*1024):.1f} MB\")\n",
    "        \n",
    "        return {\n",
    "            'status': 'success',\n",
    "            'dataset_key': dataset_key,\n",
    "            'files': downloaded_files,\n",
    "            'download_time': download_duration.total_seconds(),\n",
    "            'total_size_mb': total_size / (1024*1024),\n",
    "            'timestamp': download_end.isoformat()\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   âŒ Download failed: {str(e)}\")\n",
    "        return {\n",
    "            'status': 'failed',\n",
    "            'dataset_key': dataset_key,\n",
    "            'error': str(e),\n",
    "            'timestamp': datetime.now().isoformat()\n",
    "        }\n",
    "\n",
    "def verify_dataset_integrity(dataset_path, expected_extensions=['.csv', '.json', '.txt']):\n",
    "    \"\"\"\n",
    "    Verify dataset integrity by checking file formats and basic structure.\n",
    "    \n",
    "    Args:\n",
    "        dataset_path (Path): Path to dataset directory\n",
    "        expected_extensions (list): Expected file extensions\n",
    "    \n",
    "    Returns:\n",
    "        dict: Integrity check results\n",
    "    \"\"\"\n",
    "    files = list(dataset_path.glob('*'))\n",
    "    data_files = [f for f in files if f.suffix.lower() in expected_extensions]\n",
    "    \n",
    "    integrity_report = {\n",
    "        'total_files': len(files),\n",
    "        'data_files': len(data_files),\n",
    "        'file_extensions': list(set(f.suffix.lower() for f in files)),\n",
    "        'largest_file': None,\n",
    "        'total_size_mb': 0\n",
    "    }\n",
    "    \n",
    "    if files:\n",
    "        file_sizes = [(f, f.stat().st_size) for f in files if f.is_file()]\n",
    "        if file_sizes:\n",
    "            largest_file, largest_size = max(file_sizes, key=lambda x: x[1])\n",
    "            integrity_report['largest_file'] = {\n",
    "                'name': largest_file.name,\n",
    "                'size_mb': largest_size / (1024*1024)\n",
    "            }\n",
    "            integrity_report['total_size_mb'] = sum(size for _, size in file_sizes) / (1024*1024)\n",
    "    \n",
    "    return integrity_report\n",
    "\n",
    "print(\"âœ“ Dataset download functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e3cb5b",
   "metadata": {},
   "source": [
    "## 4. Primary Dataset Download - Temperature Humidity Pressure Illuminance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "397ec14f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¯ Downloading Primary Dataset (Priority 1)\n",
      "==================================================\n",
      "\n",
      "ğŸ”„ Processing: Temperature Humidity Pressure Illuminance Dataset\n",
      "   Source: patrickfleith/temperature-humidity-pressure-illuminance\n",
      "   Target: G:\\CSE Weather Model\\data\\raw\\primary_environmental\n",
      "   ğŸ“¥ Downloading...\n",
      "Dataset URL: https://www.kaggle.com/datasets/patrickfleith/temperature-humidity-pressure-illuminance\n",
      "   âœ“ Download completed in 4.2 seconds\n",
      "   ğŸ“ Files downloaded: 1\n",
      "   ğŸ’¾ Total size: 30.1 MB\n",
      "\n",
      "ğŸ“Š Primary Dataset Integrity Report:\n",
      "   Total files: 1\n",
      "   Data files: 1\n",
      "   File types: ['.csv']\n",
      "   Total size: 30.1 MB\n",
      "   Largest file: DATA-large.CSV (30.1 MB)\n",
      "   âœ“ Download completed in 4.2 seconds\n",
      "   ğŸ“ Files downloaded: 1\n",
      "   ğŸ’¾ Total size: 30.1 MB\n",
      "\n",
      "ğŸ“Š Primary Dataset Integrity Report:\n",
      "   Total files: 1\n",
      "   Data files: 1\n",
      "   File types: ['.csv']\n",
      "   Total size: 30.1 MB\n",
      "   Largest file: DATA-large.CSV (30.1 MB)\n"
     ]
    }
   ],
   "source": [
    "# Download primary dataset - matches ESP32-S3 sensor configuration\n",
    "print(\"ğŸ¯ Downloading Primary Dataset (Priority 1)\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "primary_result = download_dataset('primary_dataset', DATASETS_CONFIG['primary_dataset'])\n",
    "\n",
    "if primary_result['status'] in ['success', 'exists']:\n",
    "    primary_path = RAW_DATA_DIR / DATASETS_CONFIG['primary_dataset']['target_dir']\n",
    "    primary_integrity = verify_dataset_integrity(primary_path)\n",
    "    \n",
    "    print(f\"\\nğŸ“Š Primary Dataset Integrity Report:\")\n",
    "    print(f\"   Total files: {primary_integrity['total_files']}\")\n",
    "    print(f\"   Data files: {primary_integrity['data_files']}\")\n",
    "    print(f\"   File types: {primary_integrity['file_extensions']}\")\n",
    "    print(f\"   Total size: {primary_integrity['total_size_mb']:.1f} MB\")\n",
    "    \n",
    "    if primary_integrity['largest_file']:\n",
    "        print(f\"   Largest file: {primary_integrity['largest_file']['name']} ({primary_integrity['largest_file']['size_mb']:.1f} MB)\")\n",
    "    \n",
    "    # Store results for inventory\n",
    "    download_results = {'primary_dataset': primary_result}\n",
    "    integrity_results = {'primary_dataset': primary_integrity}\n",
    "    \n",
    "else:\n",
    "    print(f\"âš ï¸  Primary dataset download failed. This is critical for the project.\")\n",
    "    download_results = {'primary_dataset': primary_result}\n",
    "    integrity_results = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e37b8eff",
   "metadata": {},
   "source": [
    "## 5. Environmental Sensor Telemetry Data (132K Records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b04116fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“¡ Downloading Environmental Sensor Telemetry Dataset (Priority 2)\n",
      "============================================================\n",
      "\n",
      "ğŸ”„ Processing: Environmental Sensor Telemetry Data (132K)\n",
      "   Source: garystafford/environmental-sensor-data-132k\n",
      "   Target: G:\\CSE Weather Model\\data\\raw\\environmental_telemetry\n",
      "   ğŸ“¥ Downloading...\n",
      "Dataset URL: https://www.kaggle.com/datasets/garystafford/environmental-sensor-data-132k\n",
      "   âœ“ Download completed in 5.2 seconds\n",
      "   ğŸ“ Files downloaded: 1\n",
      "   ğŸ’¾ Total size: 59.1 MB\n",
      "\n",
      "ğŸ“Š Telemetry Dataset Integrity Report:\n",
      "   Total files: 1\n",
      "   Data files: 1\n",
      "   File types: ['.csv']\n",
      "   Total size: 59.1 MB\n",
      "   Largest file: iot_telemetry_data.csv (59.1 MB)\n",
      "\n",
      "ğŸ¯ Expected ~132K records for enhanced training dataset diversity\n",
      "   âœ“ Download completed in 5.2 seconds\n",
      "   ğŸ“ Files downloaded: 1\n",
      "   ğŸ’¾ Total size: 59.1 MB\n",
      "\n",
      "ğŸ“Š Telemetry Dataset Integrity Report:\n",
      "   Total files: 1\n",
      "   Data files: 1\n",
      "   File types: ['.csv']\n",
      "   Total size: 59.1 MB\n",
      "   Largest file: iot_telemetry_data.csv (59.1 MB)\n",
      "\n",
      "ğŸ¯ Expected ~132K records for enhanced training dataset diversity\n"
     ]
    }
   ],
   "source": [
    "# Download large-scale environmental telemetry dataset\n",
    "print(\"ğŸ“¡ Downloading Environmental Sensor Telemetry Dataset (Priority 2)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "telemetry_result = download_dataset('telemetry_dataset', DATASETS_CONFIG['telemetry_dataset'])\n",
    "download_results['telemetry_dataset'] = telemetry_result\n",
    "\n",
    "if telemetry_result['status'] in ['success', 'exists']:\n",
    "    telemetry_path = RAW_DATA_DIR / DATASETS_CONFIG['telemetry_dataset']['target_dir']\n",
    "    telemetry_integrity = verify_dataset_integrity(telemetry_path)\n",
    "    integrity_results['telemetry_dataset'] = telemetry_integrity\n",
    "    \n",
    "    print(f\"\\nğŸ“Š Telemetry Dataset Integrity Report:\")\n",
    "    print(f\"   Total files: {telemetry_integrity['total_files']}\")\n",
    "    print(f\"   Data files: {telemetry_integrity['data_files']}\")\n",
    "    print(f\"   File types: {telemetry_integrity['file_extensions']}\")\n",
    "    print(f\"   Total size: {telemetry_integrity['total_size_mb']:.1f} MB\")\n",
    "    \n",
    "    if telemetry_integrity['largest_file']:\n",
    "        print(f\"   Largest file: {telemetry_integrity['largest_file']['name']} ({telemetry_integrity['largest_file']['size_mb']:.1f} MB)\")\n",
    "        \n",
    "    print(f\"\\nğŸ¯ Expected ~132K records for enhanced training dataset diversity\")\n",
    "else:\n",
    "    print(f\"âš ï¸  Telemetry dataset download failed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a35979a",
   "metadata": {},
   "source": [
    "## 6. Complementary Datasets - BME280, Indoor Environment, and IoT Temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a8b4f6af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸŒ¡ï¸  Downloading BME280-Specific Dataset (Priority 3)\n",
      "==================================================\n",
      "\n",
      "ğŸ”„ Processing: BME280 Sensor Data\n",
      "   Source: faisalawan/bme280sensordata\n",
      "   Target: G:\\CSE Weather Model\\data\\raw\\bme280_specific\n",
      "   ğŸ“¥ Downloading...\n",
      "Dataset URL: https://www.kaggle.com/datasets/faisalawan/bme280sensordata\n",
      "   âœ“ Download completed in 3.2 seconds\n",
      "   ğŸ“ Files downloaded: 1\n",
      "   ğŸ’¾ Total size: 5.2 MB\n",
      "\n",
      "ğŸ“Š BME280 Dataset Integrity Report:\n",
      "   Total files: 1\n",
      "   Data files: 1\n",
      "   Total size: 5.2 MB\n",
      "   Author: faisalawan\n",
      "\n",
      "==================================================\n",
      "ğŸ  Downloading Intelligent Indoor Environment Dataset (Priority 4)\n",
      "=======================================================\n",
      "\n",
      "ğŸ”„ Processing: Intelligent Indoor Environment Dataset\n",
      "   Source: ziya07/intelligent-indoor-environment-dataset\n",
      "   Target: G:\\CSE Weather Model\\data\\raw\\intelligent_indoor\n",
      "   ğŸ“¥ Downloading...\n",
      "Dataset URL: https://www.kaggle.com/datasets/ziya07/intelligent-indoor-environment-dataset\n",
      "   âœ“ Download completed in 3.2 seconds\n",
      "   ğŸ“ Files downloaded: 1\n",
      "   ğŸ’¾ Total size: 5.2 MB\n",
      "\n",
      "ğŸ“Š BME280 Dataset Integrity Report:\n",
      "   Total files: 1\n",
      "   Data files: 1\n",
      "   Total size: 5.2 MB\n",
      "   Author: faisalawan\n",
      "\n",
      "==================================================\n",
      "ğŸ  Downloading Intelligent Indoor Environment Dataset (Priority 4)\n",
      "=======================================================\n",
      "\n",
      "ğŸ”„ Processing: Intelligent Indoor Environment Dataset\n",
      "   Source: ziya07/intelligent-indoor-environment-dataset\n",
      "   Target: G:\\CSE Weather Model\\data\\raw\\intelligent_indoor\n",
      "   ğŸ“¥ Downloading...\n",
      "Dataset URL: https://www.kaggle.com/datasets/ziya07/intelligent-indoor-environment-dataset\n",
      "   âœ“ Download completed in 2.5 seconds\n",
      "   ğŸ“ Files downloaded: 1\n",
      "   ğŸ’¾ Total size: 0.2 MB\n",
      "\n",
      "ğŸ“Š Intelligent Indoor Dataset Integrity Report:\n",
      "   Total files: 1\n",
      "   Data files: 1\n",
      "   Total size: 0.2 MB\n",
      "   Author: ziya07\n",
      "   Sampling: 15 minutes\n",
      "\n",
      "==================================================\n",
      "ğŸŒ¡ï¸ğŸ“Š Downloading IoT Temperature Readings Dataset (Priority 5)\n",
      "=======================================================\n",
      "\n",
      "ğŸ”„ Processing: Temperature Readings: IoT Devices\n",
      "   Source: atulanandjha/temperature-readings-iot-devices\n",
      "   Target: G:\\CSE Weather Model\\data\\raw\\iot_temperature\n",
      "   ğŸ“¥ Downloading...\n",
      "Dataset URL: https://www.kaggle.com/datasets/atulanandjha/temperature-readings-iot-devices\n",
      "   âœ“ Download completed in 2.5 seconds\n",
      "   ğŸ“ Files downloaded: 1\n",
      "   ğŸ’¾ Total size: 0.2 MB\n",
      "\n",
      "ğŸ“Š Intelligent Indoor Dataset Integrity Report:\n",
      "   Total files: 1\n",
      "   Data files: 1\n",
      "   Total size: 0.2 MB\n",
      "   Author: ziya07\n",
      "   Sampling: 15 minutes\n",
      "\n",
      "==================================================\n",
      "ğŸŒ¡ï¸ğŸ“Š Downloading IoT Temperature Readings Dataset (Priority 5)\n",
      "=======================================================\n",
      "\n",
      "ğŸ”„ Processing: Temperature Readings: IoT Devices\n",
      "   Source: atulanandjha/temperature-readings-iot-devices\n",
      "   Target: G:\\CSE Weather Model\\data\\raw\\iot_temperature\n",
      "   ğŸ“¥ Downloading...\n",
      "Dataset URL: https://www.kaggle.com/datasets/atulanandjha/temperature-readings-iot-devices\n",
      "   âœ“ Download completed in 4.1 seconds\n",
      "   ğŸ“ Files downloaded: 1\n",
      "   ğŸ’¾ Total size: 6.6 MB\n",
      "\n",
      "ğŸ“Š IoT Temperature Dataset Integrity Report:\n",
      "   Total files: 1\n",
      "   Data files: 1\n",
      "   Total size: 6.6 MB\n",
      "   Author: atulanandjha\n",
      "   Type: high_frequency data\n",
      "\n",
      "âœ… All complementary datasets processed\n",
      "ğŸ“Š Total datasets configured: 5\n",
      "   âœ“ Download completed in 4.1 seconds\n",
      "   ğŸ“ Files downloaded: 1\n",
      "   ğŸ’¾ Total size: 6.6 MB\n",
      "\n",
      "ğŸ“Š IoT Temperature Dataset Integrity Report:\n",
      "   Total files: 1\n",
      "   Data files: 1\n",
      "   Total size: 6.6 MB\n",
      "   Author: atulanandjha\n",
      "   Type: high_frequency data\n",
      "\n",
      "âœ… All complementary datasets processed\n",
      "ğŸ“Š Total datasets configured: 5\n"
     ]
    }
   ],
   "source": [
    "# Download BME280-specific validation dataset\n",
    "print(\"ğŸŒ¡ï¸  Downloading BME280-Specific Dataset (Priority 3)\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "bme280_result = download_dataset('bme280_dataset', DATASETS_CONFIG['bme280_dataset'])\n",
    "download_results['bme280_dataset'] = bme280_result\n",
    "\n",
    "if bme280_result['status'] in ['success', 'exists']:\n",
    "    bme280_path = RAW_DATA_DIR / DATASETS_CONFIG['bme280_dataset']['target_dir']\n",
    "    bme280_integrity = verify_dataset_integrity(bme280_path)\n",
    "    integrity_results['bme280_dataset'] = bme280_integrity\n",
    "    \n",
    "    print(f\"\\nğŸ“Š BME280 Dataset Integrity Report:\")\n",
    "    print(f\"   Total files: {bme280_integrity['total_files']}\")\n",
    "    print(f\"   Data files: {bme280_integrity['data_files']}\")\n",
    "    print(f\"   Total size: {bme280_integrity['total_size_mb']:.1f} MB\")\n",
    "    print(f\"   Author: {DATASETS_CONFIG['bme280_dataset']['author']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "# Download Intelligent Indoor Environment Dataset\n",
    "print(\"ğŸ  Downloading Intelligent Indoor Environment Dataset (Priority 4)\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "intelligent_result = download_dataset('intelligent_indoor_dataset', DATASETS_CONFIG['intelligent_indoor_dataset'])\n",
    "download_results['intelligent_indoor_dataset'] = intelligent_result\n",
    "\n",
    "if intelligent_result['status'] in ['success', 'exists']:\n",
    "    intelligent_path = RAW_DATA_DIR / DATASETS_CONFIG['intelligent_indoor_dataset']['target_dir']\n",
    "    intelligent_integrity = verify_dataset_integrity(intelligent_path)\n",
    "    integrity_results['intelligent_indoor_dataset'] = intelligent_integrity\n",
    "    \n",
    "    print(f\"\\nğŸ“Š Intelligent Indoor Dataset Integrity Report:\")\n",
    "    print(f\"   Total files: {intelligent_integrity['total_files']}\")\n",
    "    print(f\"   Data files: {intelligent_integrity['data_files']}\")\n",
    "    print(f\"   Total size: {intelligent_integrity['total_size_mb']:.1f} MB\")\n",
    "    print(f\"   Author: {DATASETS_CONFIG['intelligent_indoor_dataset']['author']}\")\n",
    "    print(f\"   Sampling: {DATASETS_CONFIG['intelligent_indoor_dataset']['sampling_frequency']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "# Download IoT Temperature Readings Dataset\n",
    "print(\"ğŸŒ¡ï¸ğŸ“Š Downloading IoT Temperature Readings Dataset (Priority 5)\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "iot_temp_result = download_dataset('iot_temperature_dataset', DATASETS_CONFIG['iot_temperature_dataset'])\n",
    "download_results['iot_temperature_dataset'] = iot_temp_result\n",
    "\n",
    "if iot_temp_result['status'] in ['success', 'exists']:\n",
    "    iot_temp_path = RAW_DATA_DIR / DATASETS_CONFIG['iot_temperature_dataset']['target_dir']\n",
    "    iot_temp_integrity = verify_dataset_integrity(iot_temp_path)\n",
    "    integrity_results['iot_temperature_dataset'] = iot_temp_integrity\n",
    "    \n",
    "    print(f\"\\nğŸ“Š IoT Temperature Dataset Integrity Report:\")\n",
    "    print(f\"   Total files: {iot_temp_integrity['total_files']}\")\n",
    "    print(f\"   Data files: {iot_temp_integrity['data_files']}\")\n",
    "    print(f\"   Total size: {iot_temp_integrity['total_size_mb']:.1f} MB\")\n",
    "    print(f\"   Author: {DATASETS_CONFIG['iot_temperature_dataset']['author']}\")\n",
    "    print(f\"   Type: {DATASETS_CONFIG['iot_temperature_dataset']['data_type']} data\")\n",
    "\n",
    "print(f\"\\nâœ… All complementary datasets processed\")\n",
    "print(f\"ğŸ“Š Total datasets configured: {len(DATASETS_CONFIG)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa134bb",
   "metadata": {},
   "source": [
    "### âœ… Dataset Links Verification Summary\n",
    "\n",
    "**Updated with Verified Kaggle Dataset Links:**\n",
    "\n",
    "1. **Primary Dataset** (Priority 1)\n",
    "   - Name: Temperature Humidity Pressure Illuminance Dataset\n",
    "   - Link: https://www.kaggle.com/datasets/patrickfleith/temperature-humidity-pressure-illuminance\n",
    "   - Author: patrickfleith (astro__pat)\n",
    "   - Size: 4 MB, measurements every 5 seconds\n",
    "\n",
    "2. **Telemetry Dataset** (Priority 2) \n",
    "   - Name: Environmental Sensor Telemetry Data (132K)\n",
    "   - Link: https://www.kaggle.com/datasets/garystafford/environmental-sensor-data-132k\n",
    "   - Author: garystafford\n",
    "   - Records: 132,000+ sensor readings\n",
    "\n",
    "3. **BME280 Dataset** (Priority 3)\n",
    "   - Name: BME280 Sensor Data\n",
    "   - Link: https://www.kaggle.com/datasets/faisalawan/bme280sensordata\n",
    "   - Author: faisalawan\n",
    "   - Hardware: Direct BME280 sensor readings\n",
    "\n",
    "4. **Indoor Environment Dataset** (Priority 4)\n",
    "   - Name: Intelligent Indoor Environment Dataset\n",
    "   - Link: https://www.kaggle.com/datasets/ziya07/intelligent-indoor-environment-dataset\n",
    "   - Author: ziya07\n",
    "   - Frequency: Every 15 minutes\n",
    "\n",
    "5. **IoT Temperature Dataset** (Priority 5)\n",
    "   - Name: Temperature Readings: IoT Devices\n",
    "   - Link: https://www.kaggle.com/datasets/atulanandjha/temperature-readings-iot-devices\n",
    "   - Author: atulanandjha\n",
    "   - Type: High frequency Heat Index data\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "204a06a3",
   "metadata": {},
   "source": [
    "## 7. Comprehensive Data Inventory System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "262c389a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“‹ Creating Comprehensive Data Inventory\n",
      "=============================================\n",
      "âœ“ Data inventory saved to: G:\\CSE Weather Model\\data\\data_inventory.json\n",
      "\n",
      "ğŸ“Š Download Summary:\n",
      "   Total datasets configured: 5\n",
      "   Successful downloads: 5\n",
      "   Failed downloads: 0\n",
      "   Total data size: 101.23 MB\n",
      "   Total files: 5\n"
     ]
    }
   ],
   "source": [
    "# Create comprehensive data inventory\n",
    "print(\"ğŸ“‹ Creating Comprehensive Data Inventory\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "# Generate inventory timestamp\n",
    "inventory_timestamp = datetime.now().isoformat()\n",
    "\n",
    "# Compile comprehensive inventory\n",
    "data_inventory = {\n",
    "    'metadata': {\n",
    "        'creation_timestamp': inventory_timestamp,\n",
    "        'project_name': 'CSE Weather Model',\n",
    "        'phase': 'Phase 2 - Data Collection & Loading',\n",
    "        'target_sensors': ['BME280', 'AHT10', 'BH1750'],\n",
    "        'sensor_parameters': {\n",
    "            'BME280': ['temperature', 'humidity', 'pressure'],\n",
    "            'AHT10': ['temperature', 'humidity'],\n",
    "            'BH1750': ['illuminance', 'light_intensity']\n",
    "        },\n",
    "        'physical_limits': {\n",
    "            'temperature': {'min': -40, 'max': 85, 'unit': 'Â°C'},\n",
    "            'humidity': {'min': 0, 'max': 100, 'unit': '%'},\n",
    "            'pressure': {'min': 300, 'max': 1100, 'unit': 'hPa'}\n",
    "        }\n",
    "    },\n",
    "    'datasets': {},\n",
    "    'summary': {\n",
    "        'total_datasets': 0,\n",
    "        'successful_downloads': 0,\n",
    "        'failed_downloads': 0,\n",
    "        'total_size_mb': 0,\n",
    "        'total_files': 0\n",
    "    }\n",
    "}\n",
    "\n",
    "# Process each dataset\n",
    "successful_downloads = 0\n",
    "failed_downloads = 0\n",
    "total_size_mb = 0\n",
    "total_files = 0\n",
    "\n",
    "for dataset_key, config in DATASETS_CONFIG.items():\n",
    "    dataset_info = {\n",
    "        'name': config['name'],\n",
    "        'description': config['description'],\n",
    "        'kaggle_reference': config['kaggle_ref'],\n",
    "        'priority': config['priority'],\n",
    "        'target_directory': config['target_dir'],\n",
    "        'expected_parameters': config['parameters']\n",
    "    }\n",
    "    \n",
    "    # Add sensor information if available\n",
    "    if 'sensors' in config:\n",
    "        dataset_info['target_sensors'] = config['sensors']\n",
    "    \n",
    "    # Add download results\n",
    "    if dataset_key in download_results:\n",
    "        download_result = download_results[dataset_key]\n",
    "        dataset_info['download_status'] = download_result['status']\n",
    "        dataset_info['download_timestamp'] = download_result.get('timestamp')\n",
    "        \n",
    "        if download_result['status'] == 'success':\n",
    "            dataset_info['download_time_seconds'] = download_result.get('download_time')\n",
    "            dataset_info['size_mb'] = download_result.get('total_size_mb', 0)\n",
    "            successful_downloads += 1\n",
    "        elif download_result['status'] == 'exists':\n",
    "            successful_downloads += 1\n",
    "        else:\n",
    "            dataset_info['error'] = download_result.get('error')\n",
    "            failed_downloads += 1\n",
    "    \n",
    "    # Add integrity results\n",
    "    if dataset_key in integrity_results:\n",
    "        integrity_result = integrity_results[dataset_key]\n",
    "        dataset_info['integrity_check'] = integrity_result\n",
    "        total_size_mb += integrity_result['total_size_mb']\n",
    "        total_files += integrity_result['total_files']\n",
    "    \n",
    "    data_inventory['datasets'][dataset_key] = dataset_info\n",
    "\n",
    "# Update summary\n",
    "data_inventory['summary'] = {\n",
    "    'total_datasets': len(DATASETS_CONFIG),\n",
    "    'successful_downloads': successful_downloads,\n",
    "    'failed_downloads': failed_downloads,\n",
    "    'total_size_mb': round(total_size_mb, 2),\n",
    "    'total_files': total_files\n",
    "}\n",
    "\n",
    "# Save inventory to JSON file\n",
    "inventory_file = DATA_DIR / 'data_inventory.json'\n",
    "with open(inventory_file, 'w') as f:\n",
    "    json.dump(data_inventory, f, indent=2)\n",
    "\n",
    "print(f\"âœ“ Data inventory saved to: {inventory_file}\")\n",
    "\n",
    "# Display summary\n",
    "print(f\"\\nğŸ“Š Download Summary:\")\n",
    "print(f\"   Total datasets configured: {data_inventory['summary']['total_datasets']}\")\n",
    "print(f\"   Successful downloads: {data_inventory['summary']['successful_downloads']}\")\n",
    "print(f\"   Failed downloads: {data_inventory['summary']['failed_downloads']}\")\n",
    "print(f\"   Total data size: {data_inventory['summary']['total_size_mb']} MB\")\n",
    "print(f\"   Total files: {data_inventory['summary']['total_files']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078281b0",
   "metadata": {},
   "source": [
    "## 8. Preliminary Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "19a27c04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Dataset exploration function defined\n"
     ]
    }
   ],
   "source": [
    "def explore_dataset_files(dataset_path, dataset_name):\n",
    "    \"\"\"\n",
    "    Perform preliminary exploration of dataset files.\n",
    "    \n",
    "    Args:\n",
    "        dataset_path (Path): Path to dataset directory\n",
    "        dataset_name (str): Name of the dataset for display\n",
    "    \n",
    "    Returns:\n",
    "        dict: Exploration results\n",
    "    \"\"\"\n",
    "    print(f\"\\nğŸ” Exploring: {dataset_name}\")\n",
    "    print(f\"   Path: {dataset_path}\")\n",
    "    \n",
    "    if not dataset_path.exists():\n",
    "        print(f\"   âŒ Dataset directory not found\")\n",
    "        return {'status': 'not_found'}\n",
    "    \n",
    "    # Find CSV files for initial exploration\n",
    "    csv_files = list(dataset_path.glob('*.csv'))\n",
    "    \n",
    "    if not csv_files:\n",
    "        print(f\"   âš ï¸  No CSV files found\")\n",
    "        all_files = list(dataset_path.glob('*'))\n",
    "        print(f\"   ğŸ“ Available files: {[f.name for f in all_files]}\")\n",
    "        return {'status': 'no_csv', 'files': [f.name for f in all_files]}\n",
    "    \n",
    "    exploration_results = {\n",
    "        'status': 'success',\n",
    "        'csv_files': [f.name for f in csv_files],\n",
    "        'file_analyses': {}\n",
    "    }\n",
    "    \n",
    "    # Analyze each CSV file\n",
    "    for csv_file in csv_files[:3]:  # Limit to first 3 files for initial exploration\n",
    "        try:\n",
    "            print(f\"\\n   ğŸ“„ Analyzing: {csv_file.name}\")\n",
    "            \n",
    "            # Load a sample of the data\n",
    "            df_sample = pd.read_csv(csv_file, nrows=1000)  # Load first 1000 rows for exploration\n",
    "            \n",
    "            file_analysis = {\n",
    "                'file_size_mb': csv_file.stat().st_size / (1024*1024),\n",
    "                'sample_rows': len(df_sample),\n",
    "                'columns': list(df_sample.columns),\n",
    "                'column_count': len(df_sample.columns),\n",
    "                'data_types': df_sample.dtypes.to_dict(),\n",
    "                'missing_values': df_sample.isnull().sum().to_dict(),\n",
    "                'sample_data': df_sample.head(3).to_dict('records') if len(df_sample) > 0 else []\n",
    "            }\n",
    "            \n",
    "            # Try to estimate total rows\n",
    "            try:\n",
    "                total_df = pd.read_csv(csv_file)\n",
    "                file_analysis['total_rows'] = len(total_df)\n",
    "                \n",
    "                # Check for weather-related columns\n",
    "                weather_columns = []\n",
    "                for col in df_sample.columns:\n",
    "                    col_lower = col.lower()\n",
    "                    if any(param in col_lower for param in ['temp', 'humid', 'press', 'light', 'illum']):\n",
    "                        weather_columns.append(col)\n",
    "                \n",
    "                file_analysis['weather_related_columns'] = weather_columns\n",
    "                \n",
    "                # Basic statistics for numeric columns\n",
    "                numeric_cols = total_df.select_dtypes(include=[np.number]).columns\n",
    "                if len(numeric_cols) > 0:\n",
    "                    file_analysis['numeric_summary'] = total_df[numeric_cols].describe().to_dict()\n",
    "                \n",
    "                del total_df  # Free memory\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"      âš ï¸  Could not load full dataset: {str(e)}\")\n",
    "                file_analysis['total_rows'] = 'unknown'\n",
    "            \n",
    "            exploration_results['file_analyses'][csv_file.name] = file_analysis\n",
    "            \n",
    "            # Display key information\n",
    "            print(f\"      ğŸ“Š Columns: {file_analysis['column_count']}\")\n",
    "            print(f\"      ğŸ“ˆ Rows: {file_analysis.get('total_rows', 'unknown')}\")\n",
    "            print(f\"      ğŸ’¾ Size: {file_analysis['file_size_mb']:.1f} MB\")\n",
    "            \n",
    "            if file_analysis.get('weather_related_columns'):\n",
    "                print(f\"      ğŸŒ¤ï¸  Weather columns: {', '.join(file_analysis['weather_related_columns'])}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"      âŒ Analysis failed: {str(e)}\")\n",
    "            exploration_results['file_analyses'][csv_file.name] = {'error': str(e)}\n",
    "    \n",
    "    return exploration_results\n",
    "\n",
    "print(\"âœ“ Dataset exploration function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "15eee202",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” PRELIMINARY DATA EXPLORATION\n",
      "==================================================\n",
      "\n",
      "ğŸ” Exploring: Temperature Humidity Pressure Illuminance Dataset\n",
      "   Path: G:\\CSE Weather Model\\data\\raw\\primary_environmental\n",
      "\n",
      "   ğŸ“„ Analyzing: DATA-large.CSV\n",
      "      ğŸ“Š Columns: 5\n",
      "      ğŸ“ˆ Rows: 693220\n",
      "      ğŸ’¾ Size: 30.1 MB\n",
      "      ğŸŒ¤ï¸  Weather columns: temperature, humidity, pressure\n",
      "\n",
      "ğŸ” Exploring: Environmental Sensor Telemetry Data (132K)\n",
      "   Path: G:\\CSE Weather Model\\data\\raw\\environmental_telemetry\n",
      "\n",
      "   ğŸ“„ Analyzing: iot_telemetry_data.csv\n",
      "      ğŸ“Š Columns: 5\n",
      "      ğŸ“ˆ Rows: 693220\n",
      "      ğŸ’¾ Size: 30.1 MB\n",
      "      ğŸŒ¤ï¸  Weather columns: temperature, humidity, pressure\n",
      "\n",
      "ğŸ” Exploring: Environmental Sensor Telemetry Data (132K)\n",
      "   Path: G:\\CSE Weather Model\\data\\raw\\environmental_telemetry\n",
      "\n",
      "   ğŸ“„ Analyzing: iot_telemetry_data.csv\n",
      "      ğŸ“Š Columns: 9\n",
      "      ğŸ“ˆ Rows: 405184\n",
      "      ğŸ’¾ Size: 59.1 MB\n",
      "      ğŸŒ¤ï¸  Weather columns: humidity, light, temp\n",
      "\n",
      "ğŸ” Exploring: BME280 Sensor Data\n",
      "   Path: G:\\CSE Weather Model\\data\\raw\\bme280_specific\n",
      "\n",
      "   ğŸ“„ Analyzing: pandas_simple.csv\n",
      "      ğŸ“Š Columns: 9\n",
      "      ğŸ“ˆ Rows: 405184\n",
      "      ğŸ’¾ Size: 59.1 MB\n",
      "      ğŸŒ¤ï¸  Weather columns: humidity, light, temp\n",
      "\n",
      "ğŸ” Exploring: BME280 Sensor Data\n",
      "   Path: G:\\CSE Weather Model\\data\\raw\\bme280_specific\n",
      "\n",
      "   ğŸ“„ Analyzing: pandas_simple.csv\n",
      "      ğŸ“Š Columns: 5\n",
      "      ğŸ“ˆ Rows: 100700\n",
      "      ğŸ’¾ Size: 5.2 MB\n",
      "      ğŸŒ¤ï¸  Weather columns: Temperature, Humidity, Pressure\n",
      "\n",
      "ğŸ” Exploring: Intelligent Indoor Environment Dataset\n",
      "   Path: G:\\CSE Weather Model\\data\\raw\\intelligent_indoor\n",
      "\n",
      "   ğŸ“„ Analyzing: intelligent_indoor_environment_dataset.csv\n",
      "      ğŸ“Š Columns: 16\n",
      "      ğŸ“ˆ Rows: 1000\n",
      "      ğŸ’¾ Size: 0.2 MB\n",
      "      ğŸŒ¤ï¸  Weather columns: room_temperature, room_humidity, lighting_intensity, lighting_control, HVAC_temperature\n",
      "\n",
      "ğŸ” Exploring: Temperature Readings: IoT Devices\n",
      "   Path: G:\\CSE Weather Model\\data\\raw\\iot_temperature\n",
      "\n",
      "   ğŸ“„ Analyzing: IOT-temp.csv\n",
      "      ğŸ“Š Columns: 5\n",
      "      ğŸ“ˆ Rows: 100700\n",
      "      ğŸ’¾ Size: 5.2 MB\n",
      "      ğŸŒ¤ï¸  Weather columns: Temperature, Humidity, Pressure\n",
      "\n",
      "ğŸ” Exploring: Intelligent Indoor Environment Dataset\n",
      "   Path: G:\\CSE Weather Model\\data\\raw\\intelligent_indoor\n",
      "\n",
      "   ğŸ“„ Analyzing: intelligent_indoor_environment_dataset.csv\n",
      "      ğŸ“Š Columns: 16\n",
      "      ğŸ“ˆ Rows: 1000\n",
      "      ğŸ’¾ Size: 0.2 MB\n",
      "      ğŸŒ¤ï¸  Weather columns: room_temperature, room_humidity, lighting_intensity, lighting_control, HVAC_temperature\n",
      "\n",
      "ğŸ” Exploring: Temperature Readings: IoT Devices\n",
      "   Path: G:\\CSE Weather Model\\data\\raw\\iot_temperature\n",
      "\n",
      "   ğŸ“„ Analyzing: IOT-temp.csv\n",
      "      ğŸ“Š Columns: 5\n",
      "      ğŸ“ˆ Rows: 97606\n",
      "      ğŸ’¾ Size: 6.6 MB\n",
      "      ğŸŒ¤ï¸  Weather columns: temp\n",
      "\n",
      "âœ“ Exploration completed for 5 datasets\n",
      "      ğŸ“Š Columns: 5\n",
      "      ğŸ“ˆ Rows: 97606\n",
      "      ğŸ’¾ Size: 6.6 MB\n",
      "      ğŸŒ¤ï¸  Weather columns: temp\n",
      "\n",
      "âœ“ Exploration completed for 5 datasets\n"
     ]
    }
   ],
   "source": [
    "# Explore all successfully downloaded datasets\n",
    "print(\"ğŸ” PRELIMINARY DATA EXPLORATION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "exploration_results = {}\n",
    "\n",
    "for dataset_key, config in DATASETS_CONFIG.items():\n",
    "    if dataset_key in download_results and download_results[dataset_key]['status'] in ['success', 'exists']:\n",
    "        dataset_path = RAW_DATA_DIR / config['target_dir']\n",
    "        exploration_result = explore_dataset_files(dataset_path, config['name'])\n",
    "        exploration_results[dataset_key] = exploration_result\n",
    "    else:\n",
    "        print(f\"\\nâ­ï¸  Skipping {config['name']} - not available\")\n",
    "\n",
    "print(f\"\\nâœ“ Exploration completed for {len(exploration_results)} datasets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d1e8f7",
   "metadata": {},
   "source": [
    "## 9. Data Quality Assessment and Sensor Range Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fcfd9a02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Sensor range validation function defined\n",
      "\n",
      "ğŸ“‹ Target Sensor Specifications:\n",
      "   Temperature: -40-85 Â°C (BME280)\n",
      "   Humidity: 0-100 % (BME280/AHT10)\n",
      "   Pressure: 300-1100 hPa (BME280)\n",
      "   Illuminance: 0-65535 lux (BH1750)\n"
     ]
    }
   ],
   "source": [
    "# Define sensor physical limits for validation\n",
    "SENSOR_LIMITS = {\n",
    "    'temperature': {'min': -40, 'max': 85, 'unit': 'Â°C', 'sensor': 'BME280'},\n",
    "    'humidity': {'min': 0, 'max': 100, 'unit': '%', 'sensor': 'BME280/AHT10'},\n",
    "    'pressure': {'min': 300, 'max': 1100, 'unit': 'hPa', 'sensor': 'BME280'},\n",
    "    'illuminance': {'min': 0, 'max': 65535, 'unit': 'lux', 'sensor': 'BH1750'}\n",
    "}\n",
    "\n",
    "def validate_sensor_ranges(df, parameter_mappings):\n",
    "    \"\"\"\n",
    "    Validate sensor readings against physical sensor limits.\n",
    "    \n",
    "    Args:\n",
    "        df (DataFrame): Dataset to validate\n",
    "        parameter_mappings (dict): Mapping of dataset columns to sensor parameters\n",
    "    \n",
    "    Returns:\n",
    "        dict: Validation results\n",
    "    \"\"\"\n",
    "    validation_results = {}\n",
    "    \n",
    "    for column, parameter in parameter_mappings.items():\n",
    "        if column in df.columns and parameter in SENSOR_LIMITS:\n",
    "            limits = SENSOR_LIMITS[parameter]\n",
    "            values = df[column].dropna()\n",
    "            \n",
    "            if len(values) == 0:\n",
    "                continue\n",
    "                \n",
    "            validation_results[column] = {\n",
    "                'parameter': parameter,\n",
    "                'sensor': limits['sensor'],\n",
    "                'expected_range': f\"{limits['min']} - {limits['max']} {limits['unit']}\",\n",
    "                'actual_range': f\"{values.min():.2f} - {values.max():.2f}\",\n",
    "                'within_limits': (values.min() >= limits['min']) and (values.max() <= limits['max']),\n",
    "                'out_of_range_count': len(values[(values < limits['min']) | (values > limits['max'])]),\n",
    "                'total_readings': len(values),\n",
    "                'missing_values': df[column].isnull().sum()\n",
    "            }\n",
    "    \n",
    "    return validation_results\n",
    "\n",
    "print(\"âœ“ Sensor range validation function defined\")\n",
    "print(f\"\\nğŸ“‹ Target Sensor Specifications:\")\n",
    "for param, limits in SENSOR_LIMITS.items():\n",
    "    print(f\"   {param.title()}: {limits['min']}-{limits['max']} {limits['unit']} ({limits['sensor']})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5ef0fbdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ”¬ DATA QUALITY ASSESSMENT\n",
      "========================================\n",
      "\n",
      "ğŸ” Assessing: Temperature Humidity Pressure Illuminance Dataset\n",
      "   ğŸ“„ DATA-large.CSV\n",
      "      ğŸ“Š Rows: 693,220\n",
      "      ğŸ¯ Target parameters found: 4/4\n",
      "      âœ… Data completeness: 100.0%\n",
      "      âœ… temperature: 19.00 - 30.00 (expected: -40 - 85 Â°C)\n",
      "      âœ… humidity: 29.30 - 56.90 (expected: 0 - 100 %)\n",
      "      âš ï¸ pressure: 96352.68 - 100301.06 (expected: 300 - 1100 hPa)\n",
      "         ğŸš¨ 693220 readings out of range\n",
      "      âœ… illuminance: 0.00 - 632.08 (expected: 0 - 65535 lux)\n",
      "\n",
      "ğŸ” Assessing: Environmental Sensor Telemetry Data (132K)\n",
      "   ğŸ“„ iot_telemetry_data.csv\n",
      "      ğŸ“Š Rows: 693,220\n",
      "      ğŸ¯ Target parameters found: 4/4\n",
      "      âœ… Data completeness: 100.0%\n",
      "      âœ… temperature: 19.00 - 30.00 (expected: -40 - 85 Â°C)\n",
      "      âœ… humidity: 29.30 - 56.90 (expected: 0 - 100 %)\n",
      "      âš ï¸ pressure: 96352.68 - 100301.06 (expected: 300 - 1100 hPa)\n",
      "         ğŸš¨ 693220 readings out of range\n",
      "      âœ… illuminance: 0.00 - 632.08 (expected: 0 - 65535 lux)\n",
      "\n",
      "ğŸ” Assessing: Environmental Sensor Telemetry Data (132K)\n",
      "   ğŸ“„ iot_telemetry_data.csv\n",
      "      ğŸ“Š Rows: 405,184\n",
      "      ğŸ¯ Target parameters found: 3/4\n",
      "      âœ… Data completeness: 100.0%\n",
      "      âœ… humidity: 1.10 - 99.90 (expected: 0 - 100 %)\n",
      "      âœ… illuminance: 0.00 - 1.00 (expected: 0 - 65535 lux)\n",
      "      âœ… temperature: 0.00 - 30.60 (expected: -40 - 85 Â°C)\n",
      "\n",
      "ğŸ” Assessing: BME280 Sensor Data\n",
      "   ğŸ“„ pandas_simple.csv\n",
      "      ğŸ“Š Rows: 100,700\n",
      "      ğŸ¯ Target parameters found: 3/4\n",
      "      âœ… Data completeness: 100.0%\n",
      "      âš ï¸ temperature: 76.51 - 108.97 (expected: -40 - 85 Â°C)\n",
      "         ğŸš¨ 1 readings out of range\n",
      "      âœ… humidity: 18.57 - 36.78 (expected: 0 - 100 %)\n",
      "      âœ… pressure: 970.07 - 996.03 (expected: 300 - 1100 hPa)\n",
      "\n",
      "ğŸ” Assessing: Intelligent Indoor Environment Dataset\n",
      "   ğŸ“„ intelligent_indoor_environment_dataset.csv\n",
      "      ğŸ“Š Rows: 1,000\n",
      "      ğŸ¯ Target parameters found: 5/4\n",
      "      âœ… Data completeness: 100.0%\n",
      "      âœ… temperature: 20.02 - 25.00 (expected: -40 - 85 Â°C)\n",
      "      âœ… humidity: 35.05 - 49.99 (expected: 0 - 100 %)\n",
      "      âœ… illuminance: 100.00 - 499.13 (expected: 0 - 65535 lux)\n",
      "      âœ… illuminance: 0.02 - 99.92 (expected: 0 - 65535 lux)\n",
      "      âœ… temperature: 20.00 - 24.99 (expected: -40 - 85 Â°C)\n",
      "\n",
      "ğŸ” Assessing: Temperature Readings: IoT Devices\n",
      "   ğŸ“„ IOT-temp.csv\n",
      "      ğŸ“Š Rows: 405,184\n",
      "      ğŸ¯ Target parameters found: 3/4\n",
      "      âœ… Data completeness: 100.0%\n",
      "      âœ… humidity: 1.10 - 99.90 (expected: 0 - 100 %)\n",
      "      âœ… illuminance: 0.00 - 1.00 (expected: 0 - 65535 lux)\n",
      "      âœ… temperature: 0.00 - 30.60 (expected: -40 - 85 Â°C)\n",
      "\n",
      "ğŸ” Assessing: BME280 Sensor Data\n",
      "   ğŸ“„ pandas_simple.csv\n",
      "      ğŸ“Š Rows: 100,700\n",
      "      ğŸ¯ Target parameters found: 3/4\n",
      "      âœ… Data completeness: 100.0%\n",
      "      âš ï¸ temperature: 76.51 - 108.97 (expected: -40 - 85 Â°C)\n",
      "         ğŸš¨ 1 readings out of range\n",
      "      âœ… humidity: 18.57 - 36.78 (expected: 0 - 100 %)\n",
      "      âœ… pressure: 970.07 - 996.03 (expected: 300 - 1100 hPa)\n",
      "\n",
      "ğŸ” Assessing: Intelligent Indoor Environment Dataset\n",
      "   ğŸ“„ intelligent_indoor_environment_dataset.csv\n",
      "      ğŸ“Š Rows: 1,000\n",
      "      ğŸ¯ Target parameters found: 5/4\n",
      "      âœ… Data completeness: 100.0%\n",
      "      âœ… temperature: 20.02 - 25.00 (expected: -40 - 85 Â°C)\n",
      "      âœ… humidity: 35.05 - 49.99 (expected: 0 - 100 %)\n",
      "      âœ… illuminance: 100.00 - 499.13 (expected: 0 - 65535 lux)\n",
      "      âœ… illuminance: 0.02 - 99.92 (expected: 0 - 65535 lux)\n",
      "      âœ… temperature: 20.00 - 24.99 (expected: -40 - 85 Â°C)\n",
      "\n",
      "ğŸ” Assessing: Temperature Readings: IoT Devices\n",
      "   ğŸ“„ IOT-temp.csv\n",
      "      ğŸ“Š Rows: 97,606\n",
      "      ğŸ¯ Target parameters found: 1/4\n",
      "      âœ… Data completeness: 100.0%\n",
      "      âœ… temperature: 21.00 - 51.00 (expected: -40 - 85 Â°C)\n",
      "\n",
      "âœ… Quality assessment completed for 5 datasets\n",
      "      ğŸ“Š Rows: 97,606\n",
      "      ğŸ¯ Target parameters found: 1/4\n",
      "      âœ… Data completeness: 100.0%\n",
      "      âœ… temperature: 21.00 - 51.00 (expected: -40 - 85 Â°C)\n",
      "\n",
      "âœ… Quality assessment completed for 5 datasets\n"
     ]
    }
   ],
   "source": [
    "# Perform quality assessment on available datasets\n",
    "print(\"\\nğŸ”¬ DATA QUALITY ASSESSMENT\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "quality_assessment = {}\n",
    "\n",
    "# Common column mapping patterns for environmental sensor data\n",
    "COMMON_COLUMN_MAPPINGS = {\n",
    "    'temp': 'temperature', 'temperature': 'temperature', 'temp_c': 'temperature',\n",
    "    'humid': 'humidity', 'humidity': 'humidity', 'hum': 'humidity',\n",
    "    'press': 'pressure', 'pressure': 'pressure', 'pres': 'pressure',\n",
    "    'light': 'illuminance', 'illuminance': 'illuminance', 'lux': 'illuminance'\n",
    "}\n",
    "\n",
    "for dataset_key, exploration_result in exploration_results.items():\n",
    "    if exploration_result['status'] == 'success':\n",
    "        dataset_config = DATASETS_CONFIG[dataset_key]\n",
    "        dataset_path = RAW_DATA_DIR / dataset_config['target_dir']\n",
    "        \n",
    "        print(f\"\\nğŸ” Assessing: {dataset_config['name']}\")\n",
    "        \n",
    "        dataset_quality = {\n",
    "            'dataset_name': dataset_config['name'],\n",
    "            'file_assessments': {},\n",
    "            'overall_quality_score': 0,\n",
    "            'recommendations': []\n",
    "        }\n",
    "        \n",
    "        # Analyze each CSV file\n",
    "        for file_name, file_analysis in exploration_result['file_analyses'].items():\n",
    "            if 'error' in file_analysis:\n",
    "                continue\n",
    "                \n",
    "            print(f\"   ğŸ“„ {file_name}\")\n",
    "            \n",
    "            try:\n",
    "                # Load dataset for quality assessment\n",
    "                csv_path = dataset_path / file_name\n",
    "                df = pd.read_csv(csv_path)\n",
    "                \n",
    "                # Identify parameter columns\n",
    "                parameter_mappings = {}\n",
    "                for col in df.columns:\n",
    "                    col_lower = col.lower()\n",
    "                    for pattern, parameter in COMMON_COLUMN_MAPPINGS.items():\n",
    "                        if pattern in col_lower:\n",
    "                            parameter_mappings[col] = parameter\n",
    "                            break\n",
    "                \n",
    "                # Validate sensor ranges\n",
    "                validation_results = validate_sensor_ranges(df, parameter_mappings)\n",
    "                \n",
    "                # Calculate quality metrics\n",
    "                total_cells = len(df) * len(df.columns)\n",
    "                missing_cells = df.isnull().sum().sum()\n",
    "                completeness_score = ((total_cells - missing_cells) / total_cells) * 100\n",
    "                \n",
    "                # Count parameters matching our target sensors\n",
    "                target_params_found = len([p for p in parameter_mappings.values() if p in SENSOR_LIMITS])\n",
    "                target_coverage_score = (target_params_found / len(SENSOR_LIMITS)) * 100\n",
    "                \n",
    "                file_quality = {\n",
    "                    'rows': len(df),\n",
    "                    'columns': len(df.columns),\n",
    "                    'completeness_score': round(completeness_score, 2),\n",
    "                    'target_coverage_score': round(target_coverage_score, 2),\n",
    "                    'parameter_mappings': parameter_mappings,\n",
    "                    'validation_results': validation_results,\n",
    "                    'missing_values_by_column': df.isnull().sum().to_dict()\n",
    "                }\n",
    "                \n",
    "                dataset_quality['file_assessments'][file_name] = file_quality\n",
    "                \n",
    "                print(f\"      ğŸ“Š Rows: {len(df):,}\")\n",
    "                print(f\"      ğŸ¯ Target parameters found: {target_params_found}/{len(SENSOR_LIMITS)}\")\n",
    "                print(f\"      âœ… Data completeness: {completeness_score:.1f}%\")\n",
    "                \n",
    "                # Display validation results\n",
    "                for col, validation in validation_results.items():\n",
    "                    status = \"âœ…\" if validation['within_limits'] else \"âš ï¸\"\n",
    "                    print(f\"      {status} {validation['parameter']}: {validation['actual_range']} (expected: {validation['expected_range']})\")\n",
    "                    \n",
    "                    if validation['out_of_range_count'] > 0:\n",
    "                        print(f\"         ğŸš¨ {validation['out_of_range_count']} readings out of range\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"      âŒ Quality assessment failed: {str(e)}\")\n",
    "                dataset_quality['file_assessments'][file_name] = {'error': str(e)}\n",
    "        \n",
    "        # Calculate overall quality score\n",
    "        valid_assessments = [fa for fa in dataset_quality['file_assessments'].values() if 'error' not in fa]\n",
    "        if valid_assessments:\n",
    "            avg_completeness = np.mean([fa['completeness_score'] for fa in valid_assessments])\n",
    "            avg_coverage = np.mean([fa['target_coverage_score'] for fa in valid_assessments])\n",
    "            dataset_quality['overall_quality_score'] = round((avg_completeness + avg_coverage) / 2, 2)\n",
    "        \n",
    "        quality_assessment[dataset_key] = dataset_quality\n",
    "\n",
    "print(f\"\\nâœ… Quality assessment completed for {len(quality_assessment)} datasets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a23200cc",
   "metadata": {},
   "source": [
    "## 10. Final Summary and Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "41c27187",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“‹ PHASE 2 COMPLETION SUMMARY\n",
      "==================================================\n",
      "\n",
      "ğŸ¯ PROJECT OBJECTIVES STATUS:\n",
      "   âœ… Kaggle API authentication configured\n",
      "   âœ… Directory structure established\n",
      "   ğŸ“Š Datasets downloaded: 5/5\n",
      "   ğŸ’¾ Total data acquired: 101.23 MB\n",
      "   ğŸ“ Total files: 5\n",
      "\n",
      "ğŸ“ˆ DATASET ACQUISITION RESULTS:\n",
      "   âœ… Success Temperature Humidity Pressure Illuminance Dataset\n",
      "      Priority: 1 | Target: primary_environmental\n",
      "   âœ… Success Environmental Sensor Telemetry Data (132K)\n",
      "      Priority: 2 | Target: environmental_telemetry\n",
      "   âœ… Success BME280 Sensor Data\n",
      "      Priority: 3 | Target: bme280_specific\n",
      "   âœ… Success Intelligent Indoor Environment Dataset\n",
      "      Priority: 4 | Target: intelligent_indoor\n",
      "   âœ… Success Temperature Readings: IoT Devices\n",
      "      Priority: 5 | Target: iot_temperature\n",
      "\n",
      "ğŸ”¬ DATA QUALITY OVERVIEW:\n",
      "   ğŸŸ¢ Temperature Humidity Pressure Illuminance Dataset: 100.0% quality score\n",
      "      ğŸ“Š Total records: 693,220\n",
      "   ğŸŸ¢ Environmental Sensor Telemetry Data (132K): 87.5% quality score\n",
      "      ğŸ“Š Total records: 405,184\n",
      "   ğŸŸ¢ BME280 Sensor Data: 87.5% quality score\n",
      "      ğŸ“Š Total records: 100,700\n",
      "   ğŸŸ¢ Intelligent Indoor Environment Dataset: 112.5% quality score\n",
      "      ğŸ“Š Total records: 1,000\n",
      "   ğŸŸ¡ Temperature Readings: IoT Devices: 62.5% quality score\n",
      "      ğŸ“Š Total records: 97,606\n",
      "\n",
      "ğŸŒ¡ï¸  SENSOR COMPATIBILITY:\n",
      "   Target sensors: BME280, AHT10, BH1750\n",
      "   Parameters needed: Temperature, Humidity, Pressure, Illuminance\n",
      "   Physical ranges validated against sensor specifications\n",
      "\n",
      "ğŸ“ DATA ORGANIZATION:\n",
      "   ğŸ“ Raw data location: G:\\CSE Weather Model\\data\\raw\n",
      "   ğŸ“ External data location: G:\\CSE Weather Model\\data\\external\n",
      "   ğŸ“ Processed data location: G:\\CSE Weather Model\\data\\processed\n",
      "   ğŸ“ Inventory file: G:\\CSE Weather Model\\data\\data_inventory.json\n",
      "\n",
      "ğŸ”„ NEXT STEPS FOR PHASE 3:\n",
      "   1. Data preprocessing and cleaning\n",
      "   2. Dataset harmonization and combination\n",
      "   3. Feature engineering for weather prediction\n",
      "   4. Train/validation/test split preparation\n",
      "   5. Exploratory data analysis and visualization\n",
      "\n",
      "âš ï¸  IMPORTANT NOTES:\n",
      "   ğŸ” Ensure reproducibility: All team members should use same data sources\n",
      "   ğŸ“Š Data inventory saved for version tracking and documentation\n",
      "   ğŸ¯ Focus on primary dataset for initial model development\n",
      "\n",
      "==================================================\n",
      "ğŸ“‹ Phase 2 Data Collection & Loading: COMPLETED\n",
      "â±ï¸  Execution completed at: 2025-09-15 13:22:52\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Generate comprehensive summary report\n",
    "print(\"ğŸ“‹ PHASE 2 COMPLETION SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(f\"\\nğŸ¯ PROJECT OBJECTIVES STATUS:\")\n",
    "print(f\"   âœ… Kaggle API authentication configured\")\n",
    "print(f\"   âœ… Directory structure established\")\n",
    "print(f\"   ğŸ“Š Datasets downloaded: {data_inventory['summary']['successful_downloads']}/{data_inventory['summary']['total_datasets']}\")\n",
    "print(f\"   ğŸ’¾ Total data acquired: {data_inventory['summary']['total_size_mb']} MB\")\n",
    "print(f\"   ğŸ“ Total files: {data_inventory['summary']['total_files']}\")\n",
    "\n",
    "print(f\"\\nğŸ“ˆ DATASET ACQUISITION RESULTS:\")\n",
    "for dataset_key, config in DATASETS_CONFIG.items():\n",
    "    status = \"âŒ Failed\"\n",
    "    if dataset_key in download_results:\n",
    "        if download_results[dataset_key]['status'] in ['success', 'exists']:\n",
    "            status = \"âœ… Success\"\n",
    "        \n",
    "    print(f\"   {status} {config['name']}\")\n",
    "    print(f\"      Priority: {config['priority']} | Target: {config['target_dir']}\")\n",
    "\n",
    "print(f\"\\nğŸ”¬ DATA QUALITY OVERVIEW:\")\n",
    "if quality_assessment:\n",
    "    for dataset_key, quality_data in quality_assessment.items():\n",
    "        dataset_name = quality_data['dataset_name']\n",
    "        quality_score = quality_data['overall_quality_score']\n",
    "        \n",
    "        quality_icon = \"ğŸŸ¢\" if quality_score >= 80 else \"ğŸŸ¡\" if quality_score >= 60 else \"ğŸ”´\"\n",
    "        print(f\"   {quality_icon} {dataset_name}: {quality_score}% quality score\")\n",
    "        \n",
    "        # Count total records across files\n",
    "        total_records = sum(fa.get('rows', 0) for fa in quality_data['file_assessments'].values() if 'error' not in fa)\n",
    "        if total_records > 0:\n",
    "            print(f\"      ğŸ“Š Total records: {total_records:,}\")\n",
    "else:\n",
    "    print(\"   âš ï¸  No quality assessments available\")\n",
    "\n",
    "print(f\"\\nğŸŒ¡ï¸  SENSOR COMPATIBILITY:\")\n",
    "print(f\"   Target sensors: BME280, AHT10, BH1750\")\n",
    "print(f\"   Parameters needed: Temperature, Humidity, Pressure, Illuminance\")\n",
    "print(f\"   Physical ranges validated against sensor specifications\")\n",
    "\n",
    "print(f\"\\nğŸ“ DATA ORGANIZATION:\")\n",
    "print(f\"   ğŸ“ Raw data location: {RAW_DATA_DIR}\")\n",
    "print(f\"   ğŸ“ External data location: {EXTERNAL_DATA_DIR}\")\n",
    "print(f\"   ğŸ“ Processed data location: {PROCESSED_DATA_DIR}\")\n",
    "print(f\"   ğŸ“ Inventory file: {DATA_DIR / 'data_inventory.json'}\")\n",
    "\n",
    "print(f\"\\nğŸ”„ NEXT STEPS FOR PHASE 3:\")\n",
    "print(f\"   1. Data preprocessing and cleaning\")\n",
    "print(f\"   2. Dataset harmonization and combination\")\n",
    "print(f\"   3. Feature engineering for weather prediction\")\n",
    "print(f\"   4. Train/validation/test split preparation\")\n",
    "print(f\"   5. Exploratory data analysis and visualization\")\n",
    "\n",
    "print(f\"\\nâš ï¸  IMPORTANT NOTES:\")\n",
    "if data_inventory['summary']['failed_downloads'] > 0:\n",
    "    print(f\"   ğŸš¨ {data_inventory['summary']['failed_downloads']} dataset(s) failed to download\")\n",
    "    print(f\"   ğŸ“‹ Check Kaggle API credentials and dataset availability\")\n",
    "\n",
    "print(f\"   ğŸ” Ensure reproducibility: All team members should use same data sources\")\n",
    "print(f\"   ğŸ“Š Data inventory saved for version tracking and documentation\")\n",
    "print(f\"   ğŸ¯ Focus on primary dataset for initial model development\")\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 50)\n",
    "print(f\"ğŸ“‹ Phase 2 Data Collection & Loading: COMPLETED\")\n",
    "print(f\"â±ï¸  Execution completed at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"\" + \"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6bdaede4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ’¾ Execution summary saved to: G:\\CSE Weather Model\\data\\phase2_execution_summary.json\n",
      "\n",
      "ğŸš€ Ready to proceed to Phase 3: Data Preprocessing & Feature Engineering\n"
     ]
    }
   ],
   "source": [
    "# Save final execution summary\n",
    "execution_summary = {\n",
    "    'phase': 'Phase 2 - Data Collection & Loading',\n",
    "    'execution_timestamp': datetime.now().isoformat(),\n",
    "    'datasets_configured': len(DATASETS_CONFIG),\n",
    "    'successful_downloads': data_inventory['summary']['successful_downloads'],\n",
    "    'failed_downloads': data_inventory['summary']['failed_downloads'],\n",
    "    'total_data_size_mb': data_inventory['summary']['total_size_mb'],\n",
    "    'total_files': data_inventory['summary']['total_files'],\n",
    "    'quality_assessments_completed': len(quality_assessment),\n",
    "    'next_phase': 'Phase 3 - Data Preprocessing & Feature Engineering',\n",
    "    'critical_files': {\n",
    "        'data_inventory': str(DATA_DIR / 'data_inventory.json'),\n",
    "        'notebook': str(Path.cwd() / '02_data_collection_loading.ipynb')\n",
    "    }\n",
    "}\n",
    "\n",
    "summary_file = DATA_DIR / 'phase2_execution_summary.json'\n",
    "with open(summary_file, 'w') as f:\n",
    "    json.dump(execution_summary, f, indent=2)\n",
    "\n",
    "print(f\"ğŸ’¾ Execution summary saved to: {summary_file}\")\n",
    "print(f\"\\nğŸš€ Ready to proceed to Phase 3: Data Preprocessing & Feature Engineering\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "weather_ml_env (3.10.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
